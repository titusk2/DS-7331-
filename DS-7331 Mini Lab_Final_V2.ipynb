{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405ae683-efca-4ace-9f9e-2fde06d27f13",
   "metadata": {},
   "source": [
    "# DS-7331 Mini Lab - Logistic Regression and SVMs\n",
    "\n",
    "### By Ahmad Salama, Titus Karuri, Eman Meraj, Sue Camara"
   ]
  },
  {
   "cell_type": "code",
   "id": "cefbf508-0ad7-4491-b42f-5ba3f52e4dfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:36.739735100Z",
     "start_time": "2026-02-08T23:06:35.651211600Z"
    }
   },
   "source": [
    "# ---------------------------------------------------------\n",
    "# Python Library Setup: Check and Install Required Dependencies\n",
    "# ---------------------------------------------------------\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util as imp_util\n",
    "\n",
    "# List of required libraries\\packages for this notebook\n",
    "required_packages = [\n",
    "    \"pandas\",\n",
    "    \"missingno\",\n",
    "    \"seaborn\",\n",
    "    \"matplotlib\",\n",
    "    \"math\",\n",
    "    \"scikit-learn\",\n",
    "    \"numpy\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    # iterate over the required packages and if any is not installed, trigger a pip command\n",
    "    # to install.\n",
    "    if imp_util.find_spec(package) is None:\n",
    "        print(f\"Package '{package}' is not installed. Installing now...\")\n",
    "        # run the pip command\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package],\n",
    "            stdout=subprocess.DEVNULL\n",
    "        )\n",
    "        print(f\"Package '{package}' installed successfully.\\n\")\n",
    "    else:\n",
    "        print(f\"Package '{package}' is already installed.\\n\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Import required libraries\\packages\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'pandas' is already installed.\n",
      "\n",
      "Package 'missingno' is already installed.\n",
      "\n",
      "Package 'seaborn' is already installed.\n",
      "\n",
      "Package 'matplotlib' is already installed.\n",
      "\n",
      "Package 'math' is already installed.\n",
      "\n",
      "Package 'scikit-learn' is not installed. Installing now...\n",
      "Package 'scikit-learn' installed successfully.\n",
      "\n",
      "Package 'numpy' is already installed.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "aad27aa1-9d0d-4646-bf31-eaaf49563486",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:36.844109800Z",
     "start_time": "2026-02-08T23:06:36.751562800Z"
    }
   },
   "source": [
    "\n",
    "DATASET_NAME = \"default_of_credit_card_clients.csv\"\n",
    "data_path = os.path.join(os.getcwd(), \"data\", DATASET_NAME)\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(data_path, header=1)\n",
    "df.info"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of           ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  \\\n",
       "0          1      20000    2          2         1   24      2      2     -1   \n",
       "1          2     120000    2          2         2   26     -1      2      0   \n",
       "2          3      90000    2          2         2   34      0      0      0   \n",
       "3          4      50000    2          2         1   37      0      0      0   \n",
       "4          5      50000    1          2         1   57     -1      0     -1   \n",
       "...      ...        ...  ...        ...       ...  ...    ...    ...    ...   \n",
       "29995  29996     220000    1          3         1   39      0      0      0   \n",
       "29996  29997     150000    1          3         2   43     -1     -1     -1   \n",
       "29997  29998      30000    1          2         2   37      4      3      2   \n",
       "29998  29999      80000    1          3         1   41      1     -1      0   \n",
       "29999  30000      50000    1          2         1   46      0      0      0   \n",
       "\n",
       "       PAY_4  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
       "0         -1  ...          0          0          0         0       689   \n",
       "1          0  ...       3272       3455       3261         0      1000   \n",
       "2          0  ...      14331      14948      15549      1518      1500   \n",
       "3          0  ...      28314      28959      29547      2000      2019   \n",
       "4          0  ...      20940      19146      19131      2000     36681   \n",
       "...      ...  ...        ...        ...        ...       ...       ...   \n",
       "29995      0  ...      88004      31237      15980      8500     20000   \n",
       "29996     -1  ...       8979       5190          0      1837      3526   \n",
       "29997     -1  ...      20878      20582      19357         0         0   \n",
       "29998      0  ...      52774      11855      48944     85900      3409   \n",
       "29999      0  ...      36535      32428      15313      2078      1800   \n",
       "\n",
       "       PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0             0         0         0         0                           1  \n",
       "1          1000      1000         0      2000                           1  \n",
       "2          1000      1000      1000      5000                           0  \n",
       "3          1200      1100      1069      1000                           0  \n",
       "4         10000      9000       689       679                           0  \n",
       "...         ...       ...       ...       ...                         ...  \n",
       "29995      5003      3047      5000      1000                           0  \n",
       "29996      8998       129         0         0                           0  \n",
       "29997     22000      4200      2000      3100                           1  \n",
       "29998      1178      1926     52964      1804                           1  \n",
       "29999      1430      1000      1000      1000                           1  \n",
       "\n",
       "[30000 rows x 25 columns]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "8b7608e4-feba-4112-9417-1569aca862ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:36.935975400Z",
     "start_time": "2026-02-08T23:06:36.889956400Z"
    }
   },
   "source": [
    "unique = df[\"default payment next month\"].unique()\n",
    "print(unique)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "1eb40715-db92-4ed6-840c-35d11b2736c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:36.953572300Z",
     "start_time": "2026-02-08T23:06:36.938982900Z"
    }
   },
   "source": [
    "education=df[\"EDUCATION\"].unique()\n",
    "print(education)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 5 4 6 0]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "a85eb54b-67d4-48ca-8102-c6d1812af0a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:36.989895200Z",
     "start_time": "2026-02-08T23:06:36.970784500Z"
    }
   },
   "source": [
    "df= df[df[\"EDUCATION\"].isin([1,2,3,4])]"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "6882de0a-1d4e-498b-8667-370c7fd127b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:37.024730300Z",
     "start_time": "2026-02-08T23:06:36.995242500Z"
    }
   },
   "source": [
    "marriage = df[\"MARRIAGE\"].unique()\n",
    "print(marriage)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 0]\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "d7ee0c54-e8b2-4c29-8c65-640f6d3eebb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:37.039993200Z",
     "start_time": "2026-02-08T23:06:37.026725600Z"
    }
   },
   "source": [
    "df = df[df[\"MARRIAGE\"].isin([1,2,3])]"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "992c5876-34b9-459e-9a1a-66aa7cbdb91d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:37.075024200Z",
     "start_time": "2026-02-08T23:06:37.042991300Z"
    }
   },
   "source": [
    "sex = df[\"SEX\"].unique()\n",
    "print(sex)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "c57a32f4-4cad-432a-8069-1e75c3b05579",
   "metadata": {},
   "source": [
    "# SVM and Logistic Regression Modeling "
   ]
  },
  {
   "cell_type": "code",
   "id": "b3bbcabc-fe4f-4cfa-a828-294ee178c735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:37.118534200Z",
     "start_time": "2026-02-08T23:06:37.078024500Z"
    }
   },
   "source": [
    "# Split into train/test while preserving the class balance in the target\n",
    "target = \"default payment next month\"\n",
    "# Define the target vector\n",
    "x = df.drop(columns=[target])\n",
    "y = df[target].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x,\n",
    "    y,\n",
    "    test_size=0.2, # 20% held out for testing\n",
    "    random_state=0, # reproducible split\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Overall default rate in the TRAIN set\n",
    "global_mean = y_train.mean()\n",
    "print(\"overall default rate of TRAIN set: \", global_mean)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall default rate of TRAIN set:  0.2231418918918919\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "9b526af8-0ff7-4cb7-b952-a4bf0dcc7d58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T23:06:37.227848400Z",
     "start_time": "2026-02-08T23:06:37.130729700Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# 1) Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# 2) Fit logistic regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3) Predict class + probability\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "y_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# 4) Evaluate\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC:\", metrics.roc_auc_score(y_test, y_prob))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.813882790069245\n",
      "Confusion Matrix:\n",
      " [[4490  110]\n",
      " [ 992  329]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89      4600\n",
      "           1       0.75      0.25      0.37      1321\n",
      "\n",
      "    accuracy                           0.81      5921\n",
      "   macro avg       0.78      0.61      0.63      5921\n",
      "weighted avg       0.80      0.81      0.78      5921\n",
      "\n",
      "ROC AUC: 0.7348151927064478\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "id": "c1072263-60a8-4e3d-ac75-fd801242cf15",
   "metadata": {},
   "source": [
    "Our model does a good job at predicting credit card behavior and shows that about 22% of customers in the data actually default. In addition, this model also improves on this by correctly classifying about 81% of customers, which shows the model is learning meaningful patterns rather than guessing. So, when the model predicts that someone will default, it is usually correct, meaning it prioritizes avoiding false alarms over catching every risky case. Overall, the model is useful as a starting point for credit risk prediction, but it could be improved to identify more high-risk customers. This type of trade-off is common in financial risk models and can be adjusted depending on business needs."
   ]
  },
  {
   "cell_type": "code",
   "id": "cc74b0fb-5a19-4d9c-8d2c-250b5946f107",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-02-08T23:06:37.241080500Z"
    }
   },
   "source": [
    "# ========================\n",
    "# Support Vector Machine \n",
    "# ========================\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# Scale features \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "svm_model = SVC(kernel=\"linear\", C=1.0, probability=True, random_state=0)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "svm_pred = svm_model.predict(X_test_scaled)\n",
    "svm_prob = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"=== SVM (Linear Kernel) Results ===\")\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, svm_pred))\n",
    "print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(y_test, svm_pred))\n",
    "print(\"\\nClassification Report:\\n\", metrics.classification_report(y_test, svm_pred))\n",
    "print(\"ROC AUC:\", metrics.roc_auc_score(y_test, svm_prob))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3e1871d8-e16d-40b2-8148-45bfe53db918",
   "metadata": {},
   "source": [
    "# look at the support vectors\n",
    "print(svm_model.support_vectors_.shape)\n",
    "print(svm_model.support_.shape)\n",
    "print(svm_model.n_support_ )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "101912c5-1dcb-4aca-9db4-66edfc5b94fc",
   "metadata": {},
   "source": [
    " There are 11,646 support vectors and each one has 24 features."
   ]
  },
  {
   "cell_type": "code",
   "id": "e236f78a-17e3-4b13-920d-0f13a2063ee1",
   "metadata": {},
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_model.coef_)\n",
    "weights = pd.Series(svm_model.coef_[0],index=x.columns)\n",
    "weights.plot(kind='bar')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d97808b-f8b5-4566-af5d-c0be2f42df13",
   "metadata": {},
   "source": [
    " The coefficient plot shows that the linear SVMâ€™s decision boundary is driven primarily by repayment status variables (especially PAY_0 and PAY_2), while other features have near-zero coefficients, suggesting they add little additional linear separation beyond those dominant signals."
   ]
  },
  {
   "cell_type": "code",
   "id": "77fc526e-cbf6-4014-aadb-6aa404f4232a",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if target in df:\n",
    "    y = df[target].values # get the labels we want\n",
    "    X = df.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bd9fa6c-df54-4a65-82bc-93329628d43e",
   "metadata": {},
   "source": [
    "iter_num=0\n",
    "log_reg2 = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear' ) # get object\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    log_reg2.fit(X_train,y_train)  # train object\n",
    "    y_hat = log_reg2.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c09dda9-f521-47b4-b7ed-17b0ceedfe8b",
   "metadata": {},
   "source": [
    "Using cross-validation, Logistic Regression produced ~0.77 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "8927275f-b678-4737-b97d-264b025d2f3e",
   "metadata": {},
   "source": [
    "# make a dataframe of the training data\n",
    "df_tested_on = df.iloc[y_train].copy() # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_model.support_,:].copy()\n",
    "\n",
    "df_support[target] = y[svm_model.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "df[target] = y # also add it back in for the original data\n",
    "df_support.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0dbdd361-9aab-4aa0-b827-f9bef92fe3ea",
   "metadata": {},
   "source": [
    "# Advantages of each model for each classification task"
   ]
  },
  {
   "cell_type": "code",
   "id": "987d8418-206a-491d-ad8c-2e1e887cf5e3",
   "metadata": {},
   "source": [
    "# ============================\n",
    "# SVM Parameter Adjustment \n",
    "# ============================\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "C_values = [0.01, 0.1, 1]\n",
    "\n",
    "best_C = None\n",
    "best_auc = -1\n",
    "\n",
    "print(\"=== Linear SVM Tuning (C values) ===\")\n",
    "for C in C_values:\n",
    "    svm_tuned = SVC(kernel=\"linear\", C=C, probability=True, random_state=0)\n",
    "    svm_tuned.fit(X_train_scaled, y_train)\n",
    "\n",
    "    pred = svm_tuned.predict(X_test_scaled)\n",
    "    prob = svm_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    acc = metrics.accuracy_score(y_test, pred)\n",
    "    auc = metrics.roc_auc_score(y_test, prob)\n",
    "\n",
    "    print(f\"C={C:<6}  Accuracy={acc:.3f}  ROC_AUC={auc:.3f}\")\n",
    "\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_C = C\n",
    "\n",
    "print(f\"\\nBest C based on ROC AUC: {best_C} (ROC AUC = {best_auc:.3f})\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77ddfd99-9c13-44da-92de-ff887c8d59dd",
   "metadata": {},
   "source": [
    " Even though both models achieved similar accuracy, logistic regression offered a slightly better overall predictive performance, which includes a higher ROC/AUC that trained more efficiently, and was easier to explain, making it the better choice for this dataset and task. The SVM was competitive but did not outperform logistic regression here and is less interpretable."
   ]
  },
  {
   "cell_type": "code",
   "id": "3316d343-06b4-49fa-840b-b74a472e34c7",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Manually store results you already computed\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM (Linear)\"],\n",
    "    \"Accuracy\": [0.767, 0.767],\n",
    "    \"ROC AUC\": [0.502, 0.496]\n",
    "})\n",
    "\n",
    "results\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15620a50-2951-41d3-adf2-36217296c127",
   "metadata": {},
   "source": [
    "# Interpret Weights"
   ]
  },
  {
   "cell_type": "code",
   "id": "24dc8d20-18c0-4b49-84b2-11de7eb811cc",
   "metadata": {},
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": x.columns,\n",
    "    \"Coefficient\": log_reg.coef_[0]\n",
    "})\n",
    "\n",
    "coef_df.sort_values(by=\"Coefficient\", ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7db6279b-da7f-49d4-a91d-c7a6df74a185",
   "metadata": {},
   "source": [
    "increase in PAY_0  increase the log odds of default by 0.633. BILL_AMT1 is the strongest negative driver."
   ]
  },
  {
   "cell_type": "code",
   "id": "975e0cc8-f1c1-4a7d-bdf3-2d8ed8716f5b",
   "metadata": {},
   "source": [
    "weights = log_reg.coef_.T # take transpose to make a column vector\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "97ad907c-d76f-4b8e-832f-a3e5fbc4ef96",
   "metadata": {},
   "source": [
    "Features like PAY_0 / PAY_2 (repayment status) are directly tied to delinquency history, so they tend to be strongly predictive. If a variable is closely linked to the outcome, the model gives it a larger coefficient because it consistently improves separation. while negative coefficients (notably BILL_AMT1 and payment amounts PAY_AMT1/2) are most important for predicting the non-default class; overall, behavioral credit variables dominate demographic features"
   ]
  },
  {
   "cell_type": "code",
   "id": "c906f7a2-84cb-4808-b979-31f0cfeace91",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define target and predictors\n",
    "target = \"default payment next month\"\n",
    "y = df[target].astype(int)\n",
    "X = df.drop(columns=[target])\n",
    "\n",
    "# Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Create feature importance table\n",
    "coef_table = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Weight\": log_reg.coef_[0],\n",
    "    \"Importance\": np.abs(log_reg.coef_[0])\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "coef_table.head(10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30059608-d3fd-458f-a922-e34cd7587a3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T18:10:54.614892400Z",
     "start_time": "2026-02-08T18:10:54.602369500Z"
    }
   },
   "source": [
    "PAY_0 is the dominant positive driver of credit card defaults, while BILL_AMT1 and the payment amount variables (PAY_AMT1, PAY_AMT2) have large negative coefficients, meaning higher payment behavior is associated with the no-default class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7c00b-3809-4eb0-872c-124d0fc50d56",
   "metadata": {},
   "source": [
    "# Chosen support vectors"
   ]
  },
  {
   "cell_type": "code",
   "id": "baa0838a-cc2e-425e-b753-8c7bf2fde290",
   "metadata": {},
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby([target])\n",
    "df_grouped = df.groupby([target])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['PAY_0','BILL_AMT1', 'PAY_AMT2']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['default', 'no default'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['default', 'no default'])\n",
    "    plt.title(v+' (Original)')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed0089c8-bfff-4fd4-b489-3c3377b962c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    " The support vectors say that the SVM decision making is happening in the tails or non zero regions, while the raw data is dominated by the zeros and heavy skew.non-zero"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
